{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data for MNIST\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "train_kwargs = {\"batch_size\": 64}\n",
    "test_kwargs = {\"batch_size\":1000}\n",
    "dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "\n",
    "# We split the training pool into training and holdback for later sampling\n",
    "train_size = int(0.5 * len(dataset1))\n",
    "sample_from_size = len(dataset1) - train_size\n",
    "train_dataset, sample_from_dataset = torch.utils.data.random_split(dataset1, [train_size, sample_from_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our CNN to train on MNIST\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "def attach_forward_hook(model, array):\n",
    "    return model.register_forward_hook(\n",
    "        lambda model, input, output: array.append(output.data.detach().tolist())\n",
    "    )\n",
    "\n",
    "def infer(model, device, data_loader, resource_uris, label_classes, inference_classes):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target, resource_uri in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # why are we calculating loss here?\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            for resource_uri, label_class, inference_class in zip(resource_uri, target.data.detach().tolist(), pred.data.detach().flatten().tolist()):\n",
    "                resource_uris.append(resource_uri)\n",
    "                label_classes.append(str(label_class))\n",
    "                inference_classes.append(str(inference_class))\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(data_loader.dataset), 100.0 * correct / len(data_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# We modify the MNIST dataset to expose some information about the source data\n",
    "# to allow us to uniquely identify an input in a way that we can recover it later\n",
    "class CustomDataset(datasets.MNIST):\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index)\n",
    "        resource_uri = f\"{'train' if self.train else 't10k'}-images-idx3-ubyte-{index}\"\n",
    "        return img, target, resource_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.325953\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 1.400255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     11\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(output, target)\n\u001b[0;32m---> 12\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m\u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and test our model\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10== 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Determine Loss on the test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.0161, Accuracy: 29855/30000 (100%)\n",
      "\n",
      "\n",
      "Average loss: 0.0505, Accuracy: 29606/30000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Inference on all data and generate embeddings\n",
    "inference_kwargs = {\"batch_size\": 1000}\n",
    "\n",
    "train_embeddings = []\n",
    "train_resource_uris = []\n",
    "train_label_classes = []\n",
    "train_inference_classes = []\n",
    "\n",
    "sample_from_embeddings = []\n",
    "sample_from_resource_uris = []\n",
    "sample_from_label_classes = []\n",
    "sample_from_inference_classes = []\n",
    "\n",
    "train_mnist_data = CustomDataset(\"../data\", train=True, transform=transform, download=True)\n",
    "train_dataset, sample_from_dataset = torch.utils.data.random_split(train_mnist_data, [train_size, sample_from_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# from train\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset, **inference_kwargs)\n",
    "hook = attach_forward_hook(model.fc2, train_embeddings)\n",
    "infer(model, device, data_loader, train_resource_uris, train_label_classes, train_inference_classes)\n",
    "hook.remove()\n",
    "\n",
    "# from sample_from\n",
    "data_loader = torch.utils.data.DataLoader(sample_from_dataset, **inference_kwargs)\n",
    "attach_forward_hook(model.fc2, sample_from_embeddings)\n",
    "infer(model, device, data_loader, sample_from_resource_uris, sample_from_label_classes, sample_from_inference_classes)\n",
    "\n",
    "# remove one dimension from embeddings\n",
    "train_embeddings = [item for sublist in train_embeddings for item in sublist]\n",
    "sample_from_embeddings = [item for sublist in sample_from_embeddings for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma in client mode using REST to connect to remote server\n",
      "1669936805800599795000\n"
     ]
    }
   ],
   "source": [
    "from chroma.config import Settings\n",
    "api = chroma.get_api(Settings(chroma_api_impl=\"rest\",\n",
    "                              chroma_server_host=\"localhost\",\n",
    "                              chroma_server_http_port=\"8000\") )\n",
    "\n",
    "print(api.heartbeat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into Chroma\n",
    "# api = chroma.get_api()\n",
    "\n",
    "\n",
    "api.reset()\n",
    "api.set_model_space(\"mnist\")\n",
    "\n",
    "api.add(\n",
    "    embedding= train_embeddings,\n",
    "    input_uri= train_resource_uris,\n",
    "    dataset= \"train\",\n",
    "    inference_class= train_inference_classes,\n",
    "    label_class= train_label_classes,\n",
    "    model_space= \"mnist\"\n",
    ")\n",
    "api.add(\n",
    "    embedding= sample_from_embeddings,\n",
    "    input_uri= sample_from_resource_uris,\n",
    "    dataset= \"test\",\n",
    "    inference_class= sample_from_inference_classes,\n",
    "    label_class= sample_from_label_classes,\n",
    "    model_space= \"mnist\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(api.count(model_space=\"mnist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_index(model_space=\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_space                                  uuid  \\\n",
      "0       mnist  b42ae3f4-ac03-48b6-9aaf-6c446fc44981   \n",
      "1       mnist  f9e5baf0-3d3c-4b0f-bcee-6fa64cb4e2fe   \n",
      "2       mnist  d7413bff-d82b-406b-a605-c4500b5a43bd   \n",
      "3       mnist  0589dff5-f03d-4207-86eb-ec1f029d638a   \n",
      "4       mnist  5f6a34a3-f882-4ec1-836c-ab6eedf854c5   \n",
      "5       mnist  04edec1a-7fc7-43c6-be17-cb5c565aed90   \n",
      "6       mnist  de92d375-48fd-4cb0-be80-ef1292165fc9   \n",
      "7       mnist  e181ef07-d805-4956-b0f4-a1d90df26ae3   \n",
      "8       mnist  9a309430-b5c8-48ab-b7a5-6cd864a8fc5a   \n",
      "9       mnist  d66f3cb2-54b9-4b16-8f54-67b0218a3ad8   \n",
      "\n",
      "                                           embedding  \\\n",
      "0  [-16.076580047607422, -16.445514678955078, -16...   \n",
      "1  [-10.801337242126465, -17.538055419921875, -14...   \n",
      "2  [12.641288757324219, -20.95970344543457, -8.41...   \n",
      "3  [-16.89393424987793, -18.083648681640625, -14....   \n",
      "4  [-10.38875961303711, -2.6868276596069336, 8.27...   \n",
      "5  [-13.484353065490723, 9.503548622131348, -10.3...   \n",
      "6  [-16.76352882385254, -4.135776519775391, -16.0...   \n",
      "7  [-15.87321949005127, -18.828960418701172, -15....   \n",
      "8  [-20.48949432373047, -15.539388656616211, -21....   \n",
      "9  [-20.011648178100586, -14.246245384216309, -21...   \n",
      "\n",
      "                       input_uri dataset inference_class label_class  \n",
      "0  train-images-idx3-ubyte-47355    test               9           9  \n",
      "1  train-images-idx3-ubyte-37845    test               6           6  \n",
      "2   train-images-idx3-ubyte-6502    test               0           0  \n",
      "3   train-images-idx3-ubyte-3099    test               9           9  \n",
      "4  train-images-idx3-ubyte-37668    test               2           2  \n",
      "5  train-images-idx3-ubyte-35052    test               1           1  \n",
      "6  train-images-idx3-ubyte-43433    test               4           4  \n",
      "7  train-images-idx3-ubyte-22305    test               9           9  \n",
      "8  train-images-idx3-ubyte-53642    test               5           5  \n",
      "9  train-images-idx3-ubyte-15804    test               5           5  \n"
     ]
    }
   ],
   "source": [
    "print(api.fetch(limit=10, where={\"model_space\": \"mnist\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.process(training_dataset_name=\"train\", inference_dataset_name=\"test\", model_space=\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index and run ANN (commented out)\n",
    "# api.create_index()\n",
    "# results = api.get_nearest_neighbors(sample_from_embeddings[0], n_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   0\n",
      "0       train-images-idx3-ubyte-7850\n",
      "1      train-images-idx3-ubyte-19364\n",
      "2      train-images-idx3-ubyte-18676\n",
      "3      train-images-idx3-ubyte-38518\n",
      "4       train-images-idx3-ubyte-1551\n",
      "...                              ...\n",
      "13127  train-images-idx3-ubyte-57165\n",
      "13128  train-images-idx3-ubyte-45672\n",
      "13129  train-images-idx3-ubyte-33034\n",
      "13130   train-images-idx3-ubyte-9856\n",
      "13131  train-images-idx3-ubyte-25398\n",
      "\n",
      "[13132 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get results back from Chroma\n",
    "results = api.get_results(dataset_name=\"test\", n_results=15000)\n",
    "print(results)\n",
    "# sample_from_crhoma_subset = [x for x in sample_from_dataset if x[2] in [y for y in results]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 15k results\n",
    "random_sample_from_dataset = torch.utils.data.Subset(sample_from_dataset, torch.randperm(len(sample_from_dataset))[:15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/42514 (0%)]\tLoss: 2.329946\n",
      "Train Epoch: 1 [640/42514 (2%)]\tLoss: 1.503047\n",
      "Train Epoch: 1 [1280/42514 (3%)]\tLoss: 0.670194\n",
      "Train Epoch: 1 [1920/42514 (5%)]\tLoss: 0.629320\n",
      "Train Epoch: 1 [2560/42514 (6%)]\tLoss: 0.562270\n",
      "Train Epoch: 1 [3200/42514 (8%)]\tLoss: 0.569395\n",
      "Train Epoch: 1 [3840/42514 (9%)]\tLoss: 0.366921\n",
      "Train Epoch: 1 [4480/42514 (11%)]\tLoss: 0.439915\n",
      "Train Epoch: 1 [5120/42514 (12%)]\tLoss: 0.212273\n",
      "Train Epoch: 1 [5760/42514 (14%)]\tLoss: 0.615112\n",
      "Train Epoch: 1 [6400/42514 (15%)]\tLoss: 0.247997\n",
      "Train Epoch: 1 [7040/42514 (17%)]\tLoss: 0.300236\n",
      "Train Epoch: 1 [7680/42514 (18%)]\tLoss: 0.321010\n",
      "Train Epoch: 1 [8320/42514 (20%)]\tLoss: 0.230446\n",
      "Train Epoch: 1 [8960/42514 (21%)]\tLoss: 0.444813\n",
      "Train Epoch: 1 [9600/42514 (23%)]\tLoss: 0.318204\n",
      "Train Epoch: 1 [10240/42514 (24%)]\tLoss: 0.094289\n",
      "Train Epoch: 1 [10880/42514 (26%)]\tLoss: 0.115975\n",
      "Train Epoch: 1 [11520/42514 (27%)]\tLoss: 0.114810\n",
      "Train Epoch: 1 [12160/42514 (29%)]\tLoss: 0.068771\n",
      "Train Epoch: 1 [12800/42514 (30%)]\tLoss: 0.332126\n",
      "Train Epoch: 1 [13440/42514 (32%)]\tLoss: 0.207716\n",
      "Train Epoch: 1 [14080/42514 (33%)]\tLoss: 0.165023\n",
      "Train Epoch: 1 [14720/42514 (35%)]\tLoss: 0.159624\n",
      "Train Epoch: 1 [15360/42514 (36%)]\tLoss: 0.152053\n",
      "Train Epoch: 1 [16000/42514 (38%)]\tLoss: 0.057155\n",
      "Train Epoch: 1 [16640/42514 (39%)]\tLoss: 0.223289\n",
      "Train Epoch: 1 [17280/42514 (41%)]\tLoss: 0.059202\n",
      "Train Epoch: 1 [17920/42514 (42%)]\tLoss: 0.321394\n",
      "Train Epoch: 1 [18560/42514 (44%)]\tLoss: 0.116976\n",
      "Train Epoch: 1 [19200/42514 (45%)]\tLoss: 0.364280\n",
      "Train Epoch: 1 [19840/42514 (47%)]\tLoss: 0.086889\n",
      "Train Epoch: 1 [20480/42514 (48%)]\tLoss: 0.150632\n",
      "Train Epoch: 1 [21120/42514 (50%)]\tLoss: 0.249491\n",
      "Train Epoch: 1 [21760/42514 (51%)]\tLoss: 0.142696\n",
      "Train Epoch: 1 [22400/42514 (53%)]\tLoss: 0.182294\n",
      "Train Epoch: 1 [23040/42514 (54%)]\tLoss: 0.172651\n",
      "Train Epoch: 1 [23680/42514 (56%)]\tLoss: 0.064289\n",
      "Train Epoch: 1 [24320/42514 (57%)]\tLoss: 0.149348\n",
      "Train Epoch: 1 [24960/42514 (59%)]\tLoss: 0.193570\n",
      "Train Epoch: 1 [25600/42514 (60%)]\tLoss: 0.063187\n",
      "Train Epoch: 1 [26240/42514 (62%)]\tLoss: 0.052374\n",
      "Train Epoch: 1 [26880/42514 (63%)]\tLoss: 0.069239\n",
      "Train Epoch: 1 [27520/42514 (65%)]\tLoss: 0.068936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m output \u001b[39m=\u001b[39m sampled_model(data)\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(output, target)\n\u001b[1;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [3], line 15\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m     14\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m---> 15\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)\n\u001b[1;32m     16\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     17\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(x, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train from scratch on the original cut and the sampled results\n",
    "\n",
    "# Create a dataloader which is a combination of the original cut and the sampled results\n",
    "train_sampled_dataset = torch.utils.data.ConcatDataset([train_dataset, sample_from_crhoma_subset])\n",
    "train_sampled_loader = torch.utils.data.DataLoader(train_sampled_dataset, **train_kwargs)\n",
    "\n",
    "sampled_model = Net()\n",
    "optimizer = optim.Adadelta(sampled_model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "# Train and test our model\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # Train\n",
    "    sampled_model.train()\n",
    "    # emumerate through the dataloader\n",
    "    for batch_idx, (data, target, _) in enumerate(train_sampled_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = sampled_model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10== 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_sampled_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_sampled_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Determine Loss on the test set\n",
    "    sampled_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = sampled_model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(sampled_model.state_dict(), \"mnist_cnn_sampled.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/45000 (0%)]\tLoss: 2.307080\n",
      "Train Epoch: 1 [640/45000 (1%)]\tLoss: 1.711250\n",
      "Train Epoch: 1 [1280/45000 (3%)]\tLoss: 0.790761\n",
      "Train Epoch: 1 [1920/45000 (4%)]\tLoss: 0.837130\n",
      "Train Epoch: 1 [2560/45000 (6%)]\tLoss: 0.409986\n",
      "Train Epoch: 1 [3200/45000 (7%)]\tLoss: 0.376750\n",
      "Train Epoch: 1 [3840/45000 (9%)]\tLoss: 0.215362\n",
      "Train Epoch: 1 [4480/45000 (10%)]\tLoss: 0.454628\n",
      "Train Epoch: 1 [5120/45000 (11%)]\tLoss: 0.182414\n",
      "Train Epoch: 1 [5760/45000 (13%)]\tLoss: 0.224168\n",
      "Train Epoch: 1 [6400/45000 (14%)]\tLoss: 0.178480\n",
      "Train Epoch: 1 [7040/45000 (16%)]\tLoss: 0.217367\n",
      "Train Epoch: 1 [7680/45000 (17%)]\tLoss: 0.258223\n",
      "Train Epoch: 1 [8320/45000 (18%)]\tLoss: 0.229271\n",
      "Train Epoch: 1 [8960/45000 (20%)]\tLoss: 0.604636\n",
      "Train Epoch: 1 [9600/45000 (21%)]\tLoss: 0.315153\n",
      "Train Epoch: 1 [10240/45000 (23%)]\tLoss: 0.187763\n",
      "Train Epoch: 1 [10880/45000 (24%)]\tLoss: 0.211865\n",
      "Train Epoch: 1 [11520/45000 (26%)]\tLoss: 0.163631\n",
      "Train Epoch: 1 [12160/45000 (27%)]\tLoss: 0.106323\n",
      "Train Epoch: 1 [12800/45000 (28%)]\tLoss: 0.326737\n",
      "Train Epoch: 1 [13440/45000 (30%)]\tLoss: 0.218411\n",
      "Train Epoch: 1 [14080/45000 (31%)]\tLoss: 0.176637\n",
      "Train Epoch: 1 [14720/45000 (33%)]\tLoss: 0.183372\n",
      "Train Epoch: 1 [15360/45000 (34%)]\tLoss: 0.293889\n",
      "Train Epoch: 1 [16000/45000 (36%)]\tLoss: 0.104450\n",
      "Train Epoch: 1 [16640/45000 (37%)]\tLoss: 0.152254\n",
      "Train Epoch: 1 [17280/45000 (38%)]\tLoss: 0.053142\n",
      "Train Epoch: 1 [17920/45000 (40%)]\tLoss: 0.308976\n",
      "Train Epoch: 1 [18560/45000 (41%)]\tLoss: 0.081475\n",
      "Train Epoch: 1 [19200/45000 (43%)]\tLoss: 0.240799\n",
      "Train Epoch: 1 [19840/45000 (44%)]\tLoss: 0.137575\n",
      "Train Epoch: 1 [20480/45000 (45%)]\tLoss: 0.283253\n",
      "Train Epoch: 1 [21120/45000 (47%)]\tLoss: 0.177495\n",
      "Train Epoch: 1 [21760/45000 (48%)]\tLoss: 0.112674\n",
      "Train Epoch: 1 [22400/45000 (50%)]\tLoss: 0.088024\n",
      "Train Epoch: 1 [23040/45000 (51%)]\tLoss: 0.115697\n",
      "Train Epoch: 1 [23680/45000 (53%)]\tLoss: 0.105054\n",
      "Train Epoch: 1 [24320/45000 (54%)]\tLoss: 0.139346\n",
      "Train Epoch: 1 [24960/45000 (55%)]\tLoss: 0.199067\n",
      "Train Epoch: 1 [25600/45000 (57%)]\tLoss: 0.058115\n",
      "Train Epoch: 1 [26240/45000 (58%)]\tLoss: 0.053592\n",
      "Train Epoch: 1 [26880/45000 (60%)]\tLoss: 0.041628\n",
      "Train Epoch: 1 [27520/45000 (61%)]\tLoss: 0.120575\n",
      "Train Epoch: 1 [28160/45000 (62%)]\tLoss: 0.188125\n",
      "Train Epoch: 1 [28800/45000 (64%)]\tLoss: 0.149147\n",
      "Train Epoch: 1 [29440/45000 (65%)]\tLoss: 0.074382\n",
      "Train Epoch: 1 [30080/45000 (67%)]\tLoss: 0.121049\n",
      "Train Epoch: 1 [30720/45000 (68%)]\tLoss: 0.113281\n",
      "Train Epoch: 1 [31360/45000 (70%)]\tLoss: 0.150997\n",
      "Train Epoch: 1 [32000/45000 (71%)]\tLoss: 0.230023\n",
      "Train Epoch: 1 [32640/45000 (72%)]\tLoss: 0.374360\n",
      "Train Epoch: 1 [33280/45000 (74%)]\tLoss: 0.058916\n",
      "Train Epoch: 1 [33920/45000 (75%)]\tLoss: 0.119734\n",
      "Train Epoch: 1 [34560/45000 (77%)]\tLoss: 0.179492\n",
      "Train Epoch: 1 [35200/45000 (78%)]\tLoss: 0.169767\n",
      "Train Epoch: 1 [35840/45000 (80%)]\tLoss: 0.064510\n",
      "Train Epoch: 1 [36480/45000 (81%)]\tLoss: 0.091009\n",
      "Train Epoch: 1 [37120/45000 (82%)]\tLoss: 0.162690\n",
      "Train Epoch: 1 [37760/45000 (84%)]\tLoss: 0.031219\n",
      "Train Epoch: 1 [38400/45000 (85%)]\tLoss: 0.089921\n",
      "Train Epoch: 1 [39040/45000 (87%)]\tLoss: 0.110873\n",
      "Train Epoch: 1 [39680/45000 (88%)]\tLoss: 0.165658\n",
      "Train Epoch: 1 [40320/45000 (89%)]\tLoss: 0.048963\n",
      "Train Epoch: 1 [40960/45000 (91%)]\tLoss: 0.059883\n",
      "Train Epoch: 1 [41600/45000 (92%)]\tLoss: 0.030902\n",
      "Train Epoch: 1 [42240/45000 (94%)]\tLoss: 0.050571\n",
      "Train Epoch: 1 [42880/45000 (95%)]\tLoss: 0.090842\n",
      "Train Epoch: 1 [43520/45000 (97%)]\tLoss: 0.184270\n",
      "Train Epoch: 1 [44160/45000 (98%)]\tLoss: 0.063948\n",
      "Train Epoch: 1 [44800/45000 (99%)]\tLoss: 0.124724\n",
      "\n",
      "Test set: Average loss: 0.0474, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/45000 (0%)]\tLoss: 0.093533\n",
      "Train Epoch: 2 [640/45000 (1%)]\tLoss: 0.034627\n",
      "Train Epoch: 2 [1280/45000 (3%)]\tLoss: 0.112722\n",
      "Train Epoch: 2 [1920/45000 (4%)]\tLoss: 0.080032\n",
      "Train Epoch: 2 [2560/45000 (6%)]\tLoss: 0.072727\n",
      "Train Epoch: 2 [3200/45000 (7%)]\tLoss: 0.075389\n",
      "Train Epoch: 2 [3840/45000 (9%)]\tLoss: 0.042545\n",
      "Train Epoch: 2 [4480/45000 (10%)]\tLoss: 0.106903\n",
      "Train Epoch: 2 [5120/45000 (11%)]\tLoss: 0.153276\n",
      "Train Epoch: 2 [5760/45000 (13%)]\tLoss: 0.239540\n",
      "Train Epoch: 2 [6400/45000 (14%)]\tLoss: 0.037586\n",
      "Train Epoch: 2 [7040/45000 (16%)]\tLoss: 0.086277\n",
      "Train Epoch: 2 [7680/45000 (17%)]\tLoss: 0.119351\n",
      "Train Epoch: 2 [8320/45000 (18%)]\tLoss: 0.045860\n",
      "Train Epoch: 2 [8960/45000 (20%)]\tLoss: 0.298579\n",
      "Train Epoch: 2 [9600/45000 (21%)]\tLoss: 0.095016\n",
      "Train Epoch: 2 [10240/45000 (23%)]\tLoss: 0.042016\n",
      "Train Epoch: 2 [10880/45000 (24%)]\tLoss: 0.037544\n",
      "Train Epoch: 2 [11520/45000 (26%)]\tLoss: 0.089129\n",
      "Train Epoch: 2 [12160/45000 (27%)]\tLoss: 0.011393\n",
      "Train Epoch: 2 [12800/45000 (28%)]\tLoss: 0.171274\n",
      "Train Epoch: 2 [13440/45000 (30%)]\tLoss: 0.071538\n",
      "Train Epoch: 2 [14080/45000 (31%)]\tLoss: 0.048039\n",
      "Train Epoch: 2 [14720/45000 (33%)]\tLoss: 0.012220\n",
      "Train Epoch: 2 [15360/45000 (34%)]\tLoss: 0.143737\n",
      "Train Epoch: 2 [16000/45000 (36%)]\tLoss: 0.016765\n",
      "Train Epoch: 2 [16640/45000 (37%)]\tLoss: 0.091084\n",
      "Train Epoch: 2 [17280/45000 (38%)]\tLoss: 0.012326\n",
      "Train Epoch: 2 [17920/45000 (40%)]\tLoss: 0.291710\n",
      "Train Epoch: 2 [18560/45000 (41%)]\tLoss: 0.025326\n",
      "Train Epoch: 2 [19200/45000 (43%)]\tLoss: 0.089177\n",
      "Train Epoch: 2 [19840/45000 (44%)]\tLoss: 0.037563\n",
      "Train Epoch: 2 [20480/45000 (45%)]\tLoss: 0.068113\n",
      "Train Epoch: 2 [21120/45000 (47%)]\tLoss: 0.162390\n",
      "Train Epoch: 2 [21760/45000 (48%)]\tLoss: 0.093018\n",
      "Train Epoch: 2 [22400/45000 (50%)]\tLoss: 0.047408\n",
      "Train Epoch: 2 [23040/45000 (51%)]\tLoss: 0.027380\n",
      "Train Epoch: 2 [23680/45000 (53%)]\tLoss: 0.034270\n",
      "Train Epoch: 2 [24320/45000 (54%)]\tLoss: 0.085563\n",
      "Train Epoch: 2 [24960/45000 (55%)]\tLoss: 0.040725\n",
      "Train Epoch: 2 [25600/45000 (57%)]\tLoss: 0.009929\n",
      "Train Epoch: 2 [26240/45000 (58%)]\tLoss: 0.007584\n",
      "Train Epoch: 2 [26880/45000 (60%)]\tLoss: 0.044723\n",
      "Train Epoch: 2 [27520/45000 (61%)]\tLoss: 0.053920\n",
      "Train Epoch: 2 [28160/45000 (62%)]\tLoss: 0.178356\n",
      "Train Epoch: 2 [28800/45000 (64%)]\tLoss: 0.052689\n",
      "Train Epoch: 2 [29440/45000 (65%)]\tLoss: 0.049866\n",
      "Train Epoch: 2 [30080/45000 (67%)]\tLoss: 0.110314\n",
      "Train Epoch: 2 [30720/45000 (68%)]\tLoss: 0.068153\n",
      "Train Epoch: 2 [31360/45000 (70%)]\tLoss: 0.106830\n",
      "Train Epoch: 2 [32000/45000 (71%)]\tLoss: 0.158252\n",
      "Train Epoch: 2 [32640/45000 (72%)]\tLoss: 0.109523\n",
      "Train Epoch: 2 [33280/45000 (74%)]\tLoss: 0.038151\n",
      "Train Epoch: 2 [33920/45000 (75%)]\tLoss: 0.191200\n",
      "Train Epoch: 2 [34560/45000 (77%)]\tLoss: 0.100903\n",
      "Train Epoch: 2 [35200/45000 (78%)]\tLoss: 0.098619\n",
      "Train Epoch: 2 [35840/45000 (80%)]\tLoss: 0.016652\n",
      "Train Epoch: 2 [36480/45000 (81%)]\tLoss: 0.105018\n",
      "Train Epoch: 2 [37120/45000 (82%)]\tLoss: 0.069715\n",
      "Train Epoch: 2 [37760/45000 (84%)]\tLoss: 0.018239\n",
      "Train Epoch: 2 [38400/45000 (85%)]\tLoss: 0.140010\n",
      "Train Epoch: 2 [39040/45000 (87%)]\tLoss: 0.058532\n",
      "Train Epoch: 2 [39680/45000 (88%)]\tLoss: 0.114310\n",
      "Train Epoch: 2 [40320/45000 (89%)]\tLoss: 0.108970\n",
      "Train Epoch: 2 [40960/45000 (91%)]\tLoss: 0.010172\n",
      "Train Epoch: 2 [41600/45000 (92%)]\tLoss: 0.025562\n",
      "Train Epoch: 2 [42240/45000 (94%)]\tLoss: 0.064455\n",
      "Train Epoch: 2 [42880/45000 (95%)]\tLoss: 0.072257\n",
      "Train Epoch: 2 [43520/45000 (97%)]\tLoss: 0.056781\n",
      "Train Epoch: 2 [44160/45000 (98%)]\tLoss: 0.128679\n",
      "Train Epoch: 2 [44800/45000 (99%)]\tLoss: 0.117342\n",
      "\n",
      "Test set: Average loss: 0.0380, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/45000 (0%)]\tLoss: 0.159028\n",
      "Train Epoch: 3 [640/45000 (1%)]\tLoss: 0.022459\n",
      "Train Epoch: 3 [1280/45000 (3%)]\tLoss: 0.045278\n",
      "Train Epoch: 3 [1920/45000 (4%)]\tLoss: 0.060914\n",
      "Train Epoch: 3 [2560/45000 (6%)]\tLoss: 0.007796\n",
      "Train Epoch: 3 [3200/45000 (7%)]\tLoss: 0.072675\n",
      "Train Epoch: 3 [3840/45000 (9%)]\tLoss: 0.029522\n",
      "Train Epoch: 3 [4480/45000 (10%)]\tLoss: 0.038234\n",
      "Train Epoch: 3 [5120/45000 (11%)]\tLoss: 0.087163\n",
      "Train Epoch: 3 [5760/45000 (13%)]\tLoss: 0.270501\n",
      "Train Epoch: 3 [6400/45000 (14%)]\tLoss: 0.014299\n",
      "Train Epoch: 3 [7040/45000 (16%)]\tLoss: 0.033818\n",
      "Train Epoch: 3 [7680/45000 (17%)]\tLoss: 0.063662\n",
      "Train Epoch: 3 [8320/45000 (18%)]\tLoss: 0.020077\n",
      "Train Epoch: 3 [8960/45000 (20%)]\tLoss: 0.103877\n",
      "Train Epoch: 3 [9600/45000 (21%)]\tLoss: 0.151079\n",
      "Train Epoch: 3 [10240/45000 (23%)]\tLoss: 0.102441\n",
      "Train Epoch: 3 [10880/45000 (24%)]\tLoss: 0.030245\n",
      "Train Epoch: 3 [11520/45000 (26%)]\tLoss: 0.009029\n",
      "Train Epoch: 3 [12160/45000 (27%)]\tLoss: 0.003443\n",
      "Train Epoch: 3 [12800/45000 (28%)]\tLoss: 0.169252\n",
      "Train Epoch: 3 [13440/45000 (30%)]\tLoss: 0.031820\n",
      "Train Epoch: 3 [14080/45000 (31%)]\tLoss: 0.066274\n",
      "Train Epoch: 3 [14720/45000 (33%)]\tLoss: 0.037527\n",
      "Train Epoch: 3 [15360/45000 (34%)]\tLoss: 0.059634\n",
      "Train Epoch: 3 [16000/45000 (36%)]\tLoss: 0.006810\n",
      "Train Epoch: 3 [16640/45000 (37%)]\tLoss: 0.092537\n",
      "Train Epoch: 3 [17280/45000 (38%)]\tLoss: 0.007178\n",
      "Train Epoch: 3 [17920/45000 (40%)]\tLoss: 0.233231\n",
      "Train Epoch: 3 [18560/45000 (41%)]\tLoss: 0.033857\n",
      "Train Epoch: 3 [19200/45000 (43%)]\tLoss: 0.054738\n",
      "Train Epoch: 3 [19840/45000 (44%)]\tLoss: 0.023563\n",
      "Train Epoch: 3 [20480/45000 (45%)]\tLoss: 0.022992\n",
      "Train Epoch: 3 [21120/45000 (47%)]\tLoss: 0.053161\n",
      "Train Epoch: 3 [21760/45000 (48%)]\tLoss: 0.016217\n",
      "Train Epoch: 3 [22400/45000 (50%)]\tLoss: 0.045194\n",
      "Train Epoch: 3 [23040/45000 (51%)]\tLoss: 0.185068\n",
      "Train Epoch: 3 [23680/45000 (53%)]\tLoss: 0.044105\n",
      "Train Epoch: 3 [24320/45000 (54%)]\tLoss: 0.037996\n",
      "Train Epoch: 3 [24960/45000 (55%)]\tLoss: 0.023154\n",
      "Train Epoch: 3 [25600/45000 (57%)]\tLoss: 0.011347\n",
      "Train Epoch: 3 [26240/45000 (58%)]\tLoss: 0.020283\n",
      "Train Epoch: 3 [26880/45000 (60%)]\tLoss: 0.023558\n",
      "Train Epoch: 3 [27520/45000 (61%)]\tLoss: 0.054509\n",
      "Train Epoch: 3 [28160/45000 (62%)]\tLoss: 0.173141\n",
      "Train Epoch: 3 [28800/45000 (64%)]\tLoss: 0.049611\n",
      "Train Epoch: 3 [29440/45000 (65%)]\tLoss: 0.052324\n",
      "Train Epoch: 3 [30080/45000 (67%)]\tLoss: 0.016359\n",
      "Train Epoch: 3 [30720/45000 (68%)]\tLoss: 0.040452\n",
      "Train Epoch: 3 [31360/45000 (70%)]\tLoss: 0.035974\n",
      "Train Epoch: 3 [32000/45000 (71%)]\tLoss: 0.053548\n",
      "Train Epoch: 3 [32640/45000 (72%)]\tLoss: 0.025604\n",
      "Train Epoch: 3 [33280/45000 (74%)]\tLoss: 0.029645\n",
      "Train Epoch: 3 [33920/45000 (75%)]\tLoss: 0.004102\n",
      "Train Epoch: 3 [34560/45000 (77%)]\tLoss: 0.076725\n",
      "Train Epoch: 3 [35200/45000 (78%)]\tLoss: 0.092264\n",
      "Train Epoch: 3 [35840/45000 (80%)]\tLoss: 0.009681\n",
      "Train Epoch: 3 [36480/45000 (81%)]\tLoss: 0.039292\n",
      "Train Epoch: 3 [37120/45000 (82%)]\tLoss: 0.049074\n",
      "Train Epoch: 3 [37760/45000 (84%)]\tLoss: 0.008822\n",
      "Train Epoch: 3 [38400/45000 (85%)]\tLoss: 0.023850\n",
      "Train Epoch: 3 [39040/45000 (87%)]\tLoss: 0.042464\n",
      "Train Epoch: 3 [39680/45000 (88%)]\tLoss: 0.120579\n",
      "Train Epoch: 3 [40320/45000 (89%)]\tLoss: 0.089030\n",
      "Train Epoch: 3 [40960/45000 (91%)]\tLoss: 0.004857\n",
      "Train Epoch: 3 [41600/45000 (92%)]\tLoss: 0.012954\n",
      "Train Epoch: 3 [42240/45000 (94%)]\tLoss: 0.013705\n",
      "Train Epoch: 3 [42880/45000 (95%)]\tLoss: 0.023928\n",
      "Train Epoch: 3 [43520/45000 (97%)]\tLoss: 0.042745\n",
      "Train Epoch: 3 [44160/45000 (98%)]\tLoss: 0.045108\n",
      "Train Epoch: 3 [44800/45000 (99%)]\tLoss: 0.024168\n",
      "\n",
      "Test set: Average loss: 0.0372, Accuracy: 9875/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/45000 (0%)]\tLoss: 0.095028\n",
      "Train Epoch: 4 [640/45000 (1%)]\tLoss: 0.011371\n",
      "Train Epoch: 4 [1280/45000 (3%)]\tLoss: 0.044675\n",
      "Train Epoch: 4 [1920/45000 (4%)]\tLoss: 0.100785\n",
      "Train Epoch: 4 [2560/45000 (6%)]\tLoss: 0.005004\n",
      "Train Epoch: 4 [3200/45000 (7%)]\tLoss: 0.073829\n",
      "Train Epoch: 4 [3840/45000 (9%)]\tLoss: 0.009056\n",
      "Train Epoch: 4 [4480/45000 (10%)]\tLoss: 0.042115\n",
      "Train Epoch: 4 [5120/45000 (11%)]\tLoss: 0.056452\n",
      "Train Epoch: 4 [5760/45000 (13%)]\tLoss: 0.232389\n",
      "Train Epoch: 4 [6400/45000 (14%)]\tLoss: 0.013643\n",
      "Train Epoch: 4 [7040/45000 (16%)]\tLoss: 0.029045\n",
      "Train Epoch: 4 [7680/45000 (17%)]\tLoss: 0.017288\n",
      "Train Epoch: 4 [8320/45000 (18%)]\tLoss: 0.022808\n",
      "Train Epoch: 4 [8960/45000 (20%)]\tLoss: 0.038235\n",
      "Train Epoch: 4 [9600/45000 (21%)]\tLoss: 0.110219\n",
      "Train Epoch: 4 [10240/45000 (23%)]\tLoss: 0.073921\n",
      "Train Epoch: 4 [10880/45000 (24%)]\tLoss: 0.010109\n",
      "Train Epoch: 4 [11520/45000 (26%)]\tLoss: 0.002715\n",
      "Train Epoch: 4 [12160/45000 (27%)]\tLoss: 0.028691\n",
      "Train Epoch: 4 [12800/45000 (28%)]\tLoss: 0.138870\n",
      "Train Epoch: 4 [13440/45000 (30%)]\tLoss: 0.016497\n",
      "Train Epoch: 4 [14080/45000 (31%)]\tLoss: 0.017918\n",
      "Train Epoch: 4 [14720/45000 (33%)]\tLoss: 0.013946\n",
      "Train Epoch: 4 [15360/45000 (34%)]\tLoss: 0.085263\n",
      "Train Epoch: 4 [16000/45000 (36%)]\tLoss: 0.001643\n",
      "Train Epoch: 4 [16640/45000 (37%)]\tLoss: 0.006432\n",
      "Train Epoch: 4 [17280/45000 (38%)]\tLoss: 0.023963\n",
      "Train Epoch: 4 [17920/45000 (40%)]\tLoss: 0.285394\n",
      "Train Epoch: 4 [18560/45000 (41%)]\tLoss: 0.004251\n",
      "Train Epoch: 4 [19200/45000 (43%)]\tLoss: 0.044918\n",
      "Train Epoch: 4 [19840/45000 (44%)]\tLoss: 0.017200\n",
      "Train Epoch: 4 [20480/45000 (45%)]\tLoss: 0.008307\n",
      "Train Epoch: 4 [21120/45000 (47%)]\tLoss: 0.038940\n",
      "Train Epoch: 4 [21760/45000 (48%)]\tLoss: 0.050117\n",
      "Train Epoch: 4 [22400/45000 (50%)]\tLoss: 0.012453\n",
      "Train Epoch: 4 [23040/45000 (51%)]\tLoss: 0.069947\n",
      "Train Epoch: 4 [23680/45000 (53%)]\tLoss: 0.008581\n",
      "Train Epoch: 4 [24320/45000 (54%)]\tLoss: 0.015116\n",
      "Train Epoch: 4 [24960/45000 (55%)]\tLoss: 0.040353\n",
      "Train Epoch: 4 [25600/45000 (57%)]\tLoss: 0.004509\n",
      "Train Epoch: 4 [26240/45000 (58%)]\tLoss: 0.002979\n",
      "Train Epoch: 4 [26880/45000 (60%)]\tLoss: 0.004759\n",
      "Train Epoch: 4 [27520/45000 (61%)]\tLoss: 0.028477\n",
      "Train Epoch: 4 [28160/45000 (62%)]\tLoss: 0.091272\n",
      "Train Epoch: 4 [28800/45000 (64%)]\tLoss: 0.034807\n",
      "Train Epoch: 4 [29440/45000 (65%)]\tLoss: 0.019701\n",
      "Train Epoch: 4 [30080/45000 (67%)]\tLoss: 0.015151\n",
      "Train Epoch: 4 [30720/45000 (68%)]\tLoss: 0.043388\n",
      "Train Epoch: 4 [31360/45000 (70%)]\tLoss: 0.016388\n",
      "Train Epoch: 4 [32000/45000 (71%)]\tLoss: 0.103876\n",
      "Train Epoch: 4 [32640/45000 (72%)]\tLoss: 0.044843\n",
      "Train Epoch: 4 [33280/45000 (74%)]\tLoss: 0.052180\n",
      "Train Epoch: 4 [33920/45000 (75%)]\tLoss: 0.009655\n",
      "Train Epoch: 4 [34560/45000 (77%)]\tLoss: 0.046861\n",
      "Train Epoch: 4 [35200/45000 (78%)]\tLoss: 0.043642\n",
      "Train Epoch: 4 [35840/45000 (80%)]\tLoss: 0.007811\n",
      "Train Epoch: 4 [36480/45000 (81%)]\tLoss: 0.041325\n",
      "Train Epoch: 4 [37120/45000 (82%)]\tLoss: 0.041017\n",
      "Train Epoch: 4 [37760/45000 (84%)]\tLoss: 0.006055\n",
      "Train Epoch: 4 [38400/45000 (85%)]\tLoss: 0.006654\n",
      "Train Epoch: 4 [39040/45000 (87%)]\tLoss: 0.103911\n",
      "Train Epoch: 4 [39680/45000 (88%)]\tLoss: 0.045520\n",
      "Train Epoch: 4 [40320/45000 (89%)]\tLoss: 0.012045\n",
      "Train Epoch: 4 [40960/45000 (91%)]\tLoss: 0.018689\n",
      "Train Epoch: 4 [41600/45000 (92%)]\tLoss: 0.007026\n",
      "Train Epoch: 4 [42240/45000 (94%)]\tLoss: 0.007538\n",
      "Train Epoch: 4 [42880/45000 (95%)]\tLoss: 0.032268\n",
      "Train Epoch: 4 [43520/45000 (97%)]\tLoss: 0.029936\n",
      "Train Epoch: 4 [44160/45000 (98%)]\tLoss: 0.037381\n",
      "Train Epoch: 4 [44800/45000 (99%)]\tLoss: 0.020638\n",
      "\n",
      "Test set: Average loss: 0.0334, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/45000 (0%)]\tLoss: 0.023657\n",
      "Train Epoch: 5 [640/45000 (1%)]\tLoss: 0.011843\n",
      "Train Epoch: 5 [1280/45000 (3%)]\tLoss: 0.077065\n",
      "Train Epoch: 5 [1920/45000 (4%)]\tLoss: 0.085586\n",
      "Train Epoch: 5 [2560/45000 (6%)]\tLoss: 0.020585\n",
      "Train Epoch: 5 [3200/45000 (7%)]\tLoss: 0.035018\n",
      "Train Epoch: 5 [3840/45000 (9%)]\tLoss: 0.029960\n",
      "Train Epoch: 5 [4480/45000 (10%)]\tLoss: 0.068256\n",
      "Train Epoch: 5 [5120/45000 (11%)]\tLoss: 0.060751\n",
      "Train Epoch: 5 [5760/45000 (13%)]\tLoss: 0.118881\n",
      "Train Epoch: 5 [6400/45000 (14%)]\tLoss: 0.013052\n",
      "Train Epoch: 5 [7040/45000 (16%)]\tLoss: 0.010124\n",
      "Train Epoch: 5 [7680/45000 (17%)]\tLoss: 0.003002\n",
      "Train Epoch: 5 [8320/45000 (18%)]\tLoss: 0.026300\n",
      "Train Epoch: 5 [8960/45000 (20%)]\tLoss: 0.050002\n",
      "Train Epoch: 5 [9600/45000 (21%)]\tLoss: 0.042279\n",
      "Train Epoch: 5 [10240/45000 (23%)]\tLoss: 0.024582\n",
      "Train Epoch: 5 [10880/45000 (24%)]\tLoss: 0.005855\n",
      "Train Epoch: 5 [11520/45000 (26%)]\tLoss: 0.015429\n",
      "Train Epoch: 5 [12160/45000 (27%)]\tLoss: 0.001863\n",
      "Train Epoch: 5 [12800/45000 (28%)]\tLoss: 0.089783\n",
      "Train Epoch: 5 [13440/45000 (30%)]\tLoss: 0.019296\n",
      "Train Epoch: 5 [14080/45000 (31%)]\tLoss: 0.016151\n",
      "Train Epoch: 5 [14720/45000 (33%)]\tLoss: 0.005414\n",
      "Train Epoch: 5 [15360/45000 (34%)]\tLoss: 0.003268\n",
      "Train Epoch: 5 [16000/45000 (36%)]\tLoss: 0.027159\n",
      "Train Epoch: 5 [16640/45000 (37%)]\tLoss: 0.017448\n",
      "Train Epoch: 5 [17280/45000 (38%)]\tLoss: 0.005362\n",
      "Train Epoch: 5 [17920/45000 (40%)]\tLoss: 0.210934\n",
      "Train Epoch: 5 [18560/45000 (41%)]\tLoss: 0.005044\n",
      "Train Epoch: 5 [19200/45000 (43%)]\tLoss: 0.069955\n",
      "Train Epoch: 5 [19840/45000 (44%)]\tLoss: 0.006034\n",
      "Train Epoch: 5 [20480/45000 (45%)]\tLoss: 0.022888\n",
      "Train Epoch: 5 [21120/45000 (47%)]\tLoss: 0.055970\n",
      "Train Epoch: 5 [21760/45000 (48%)]\tLoss: 0.103911\n",
      "Train Epoch: 5 [22400/45000 (50%)]\tLoss: 0.035922\n",
      "Train Epoch: 5 [23040/45000 (51%)]\tLoss: 0.057368\n",
      "Train Epoch: 5 [23680/45000 (53%)]\tLoss: 0.024692\n",
      "Train Epoch: 5 [24320/45000 (54%)]\tLoss: 0.007918\n",
      "Train Epoch: 5 [24960/45000 (55%)]\tLoss: 0.011791\n",
      "Train Epoch: 5 [25600/45000 (57%)]\tLoss: 0.000769\n",
      "Train Epoch: 5 [26240/45000 (58%)]\tLoss: 0.003552\n",
      "Train Epoch: 5 [26880/45000 (60%)]\tLoss: 0.001642\n",
      "Train Epoch: 5 [27520/45000 (61%)]\tLoss: 0.032389\n",
      "Train Epoch: 5 [28160/45000 (62%)]\tLoss: 0.078508\n",
      "Train Epoch: 5 [28800/45000 (64%)]\tLoss: 0.010536\n",
      "Train Epoch: 5 [29440/45000 (65%)]\tLoss: 0.013124\n",
      "Train Epoch: 5 [30080/45000 (67%)]\tLoss: 0.014687\n",
      "Train Epoch: 5 [30720/45000 (68%)]\tLoss: 0.064846\n",
      "Train Epoch: 5 [31360/45000 (70%)]\tLoss: 0.014178\n",
      "Train Epoch: 5 [32000/45000 (71%)]\tLoss: 0.033180\n",
      "Train Epoch: 5 [32640/45000 (72%)]\tLoss: 0.021480\n",
      "Train Epoch: 5 [33280/45000 (74%)]\tLoss: 0.010449\n",
      "Train Epoch: 5 [33920/45000 (75%)]\tLoss: 0.037742\n",
      "Train Epoch: 5 [34560/45000 (77%)]\tLoss: 0.040664\n",
      "Train Epoch: 5 [35200/45000 (78%)]\tLoss: 0.025295\n",
      "Train Epoch: 5 [35840/45000 (80%)]\tLoss: 0.007531\n",
      "Train Epoch: 5 [36480/45000 (81%)]\tLoss: 0.046729\n",
      "Train Epoch: 5 [37120/45000 (82%)]\tLoss: 0.009853\n",
      "Train Epoch: 5 [37760/45000 (84%)]\tLoss: 0.003116\n",
      "Train Epoch: 5 [38400/45000 (85%)]\tLoss: 0.023790\n",
      "Train Epoch: 5 [39040/45000 (87%)]\tLoss: 0.009425\n",
      "Train Epoch: 5 [39680/45000 (88%)]\tLoss: 0.033856\n",
      "Train Epoch: 5 [40320/45000 (89%)]\tLoss: 0.005934\n",
      "Train Epoch: 5 [40960/45000 (91%)]\tLoss: 0.001800\n",
      "Train Epoch: 5 [41600/45000 (92%)]\tLoss: 0.003976\n",
      "Train Epoch: 5 [42240/45000 (94%)]\tLoss: 0.068319\n",
      "Train Epoch: 5 [42880/45000 (95%)]\tLoss: 0.068909\n",
      "Train Epoch: 5 [43520/45000 (97%)]\tLoss: 0.022379\n",
      "Train Epoch: 5 [44160/45000 (98%)]\tLoss: 0.033395\n",
      "Train Epoch: 5 [44800/45000 (99%)]\tLoss: 0.005756\n",
      "\n",
      "Test set: Average loss: 0.0315, Accuracy: 9899/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train from scratch on the original cut and the sampled results\n",
    "\n",
    "# Create a dataloader which is a combination of the original cut and the sampled results\n",
    "train_random_dataset = torch.utils.data.ConcatDataset([train_dataset, random_sample_from_dataset])\n",
    "train_random_loader = torch.utils.data.DataLoader(train_random_dataset, **train_kwargs)\n",
    "\n",
    "random_model = Net()\n",
    "optimizer = optim.Adadelta(random_model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "# Train and test our model\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # Train\n",
    "    random_model.train()\n",
    "    # emumerate through the dataloader\n",
    "    for batch_idx, (data, target, _) in enumerate(train_random_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = random_model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10== 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_random_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_random_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Determine Loss on the test set\n",
    "    random_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = random_model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(random_model.state_dict(), \"mnist_cnn_random.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cee379e823f0be65cc8e76dec88cd29dba28d635fb42abca317c93103c7b3ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
